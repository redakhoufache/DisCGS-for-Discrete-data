{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b1d3735-773b-4873-b817-c0c2aac56133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48602724-3c0d-4b22-93eb-419d52fb855a",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53e445eb-e61f-4ed4-9e69-f93af3d1518e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/reda/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 630000/630000 [02:52<00:00, 3660.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Output saved to: data.txt\n"
     ]
    }
   ],
   "source": [
    "# === CONFIG ===\n",
    "train_csv = \"train.csv\"\n",
    "test_csv = \"test.csv\"\n",
    "output_file = \"data.txt\"\n",
    "\n",
    "# === Setup ===\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "tqdm.pandas()\n",
    "\n",
    "# === Load and Concatenate Train + Test ===\n",
    "train_df = pd.read_csv(train_csv)\n",
    "test_df = pd.read_csv(test_csv)\n",
    "df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# === Shuffle rows of the DataFrame ===\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# === Preprocessing Function ===\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    filtered = [stemmer.stem(w) for w in words if w not in stop_words]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "# === Apply Preprocessing ===\n",
    "df['text'] = (df['title'] + \" \" + df['content']).progress_apply(preprocess)\n",
    "\n",
    "# === Write to Output File ===\n",
    "with open(output_file, 'w') as f_out:\n",
    "    for _, row in df.iterrows():\n",
    "        record = {\n",
    "            \"text\": row['text'],\n",
    "            \"cluster\": int(row['label'])  # use original label\n",
    "        }\n",
    "        f_out.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "print(f\"Preprocessing complete. Output saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082f2a77-8175-4630-91cb-61905d1d935f",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "510035f7-3ae2-4d52-9238-a58a3a68a5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents (n): 630000\n",
      "Number of clusters (K): 14\n",
      "Vocabulary size: 666482\n",
      "Average document length: 32.81 tokens\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "clusters = []\n",
    "\n",
    "with open(\"data.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        record = json.loads(line)\n",
    "        texts.append(record[\"text\"])\n",
    "        clusters.append(record[\"cluster\"])\n",
    "\n",
    "n = len(texts)\n",
    "K = len(set(clusters))\n",
    "\n",
    "# tokenize documents (split by whitespace, since your texts are preprocessed)\n",
    "tokenized_docs = [doc.split() for doc in texts]\n",
    "\n",
    "# vocabulary is the set of all unique words\n",
    "vocab = set(word for doc in tokenized_docs for word in doc)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# average length of documents (number of tokens)\n",
    "avg_len = sum(len(doc) for doc in tokenized_docs) / n\n",
    "\n",
    "print(f\"Number of documents (n): {n}\")\n",
    "print(f\"Number of clusters (K): {K}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Average document length: {avg_len:.2f} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08eea96-6826-4187-8e42-33c8f1eed10e",
   "metadata": {},
   "source": [
    "# Extract sub documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c2b5394-305a-4986-8f01-9b7997f6f828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 100000 documents\n",
      "Minimal combined vocabulary size (heuristic): 116154\n",
      "Average document length: 11.37 tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_file = \"data.txt\"\n",
    "min_docs = 100000  # fixed number of documents you want\n",
    "\n",
    "texts = []\n",
    "clusters = []\n",
    "\n",
    "with open(input_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        record = json.loads(line)\n",
    "        texts.append(record[\"text\"])\n",
    "        clusters.append(record[\"cluster\"])\n",
    "\n",
    "tokenized_docs = [doc.split() for doc in texts]\n",
    "doc_vocab_sizes = [len(set(doc)) for doc in tokenized_docs]\n",
    "\n",
    "docs_with_stats = list(zip(tokenized_docs, clusters, doc_vocab_sizes))\n",
    "docs_with_stats_sorted = sorted(docs_with_stats, key=lambda x: x[2])\n",
    "\n",
    "# Pick first min_docs documents with smallest vocab sizes\n",
    "selected_docs = [doc for doc, _, _ in docs_with_stats_sorted[:min_docs]]\n",
    "selected_clusters = [cluster for _, cluster, _ in docs_with_stats_sorted[:min_docs]]\n",
    "\n",
    "# Compute combined vocab size\n",
    "combined_vocab = set(word for doc in selected_docs for word in doc)\n",
    "combined_vocab_size = len(combined_vocab)\n",
    "\n",
    "# Average doc length\n",
    "avg_len = sum(len(doc) for doc in selected_docs) / min_docs\n",
    "\n",
    "print(f\"Selected {min_docs} documents\")\n",
    "print(f\"Minimal combined vocabulary size (heuristic): {combined_vocab_size}\")\n",
    "print(f\"Average document length: {avg_len:.2f} tokens\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
